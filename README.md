### Corporate-Policy Sample Corpus (PhaseÂ 2)

Include 15 publicly available **HR, ITâ€‘security, supplier and ethics policies** drawn from technology, finance, retail, healthcare, energy, consulting and foodâ€‘&â€‘beverage firms.
Because every organisationâ€”regardless of sectorâ€”must publish and enforce policies such as *Codes of Conduct* ðŸ”—Â \[1], *Remoteâ€‘Work rules* ðŸ”—Â \[2] and *Supplier Codes of Conduct* ðŸ”—Â \[3], a RAG chatbot trained on this diverse but common genre remains **sectorâ€‘agnostic and reusable across clients**.
The raw PDFs live in `/data/raw/`, are textâ€‘selectable, free for internal redistribution, and named `company_topic_YYYY.pdf` for traceability.

#### PhaseÂ 3Â â€“ DocumentÂ â†’Â ChunkÂ â†’Â EmbeddingÂ pipeline

Run **`make ingest`** whenever you add or update PDFs in `/data/raw/`.
The target calls `python ingest.py --pdf_dir data/raw`, which

1. crawls every PDF,
2. splits text intoÂ â‰ˆ500â€‘token chunks with 50â€‘token overlap,
3. generates dense *intfloat/e5â€‘large* embeddings, and
4. writes vectors + metadata into an embedded Qdrant collection.

The persisted vector store lives in `.storage/` (or `.qdrant/`)â€”both are gitâ€‘ignored and can be rebuilt deterministically at any time.

---

### PhaseÂ 4Â â€“ LLM Setup (Mistralâ€‘7Bâ€‘Instruct, 4â€‘bitÂ GPTQ)

> *Goal:* get a local, quantised Mistralâ€‘7B that loads in seconds and is ready for the RAG chain.

#### 0Â Â Prerequisites

* **PythonÂ 3.11** (`conda activate ragbot` or `source venv/bin/activate`).
* GPU with **CUDAÂ 12.1** driver (CPU fallback works but is slow).
* No compiler requiredâ€”preâ€‘built wheels include all CUDA kernels.
* `requirements.txt` now pins:

  ```text
  torch==2.5.1+cu121
  torchvision==0.20.1+cu121
  torchaudio==2.5.1+cu121
  optimum[gptq]>=1.24.0
  gptqmodel>=2.2.0
  huggingface_hub[cli]>=0.31.0
  ```

  Run `pip install -r requirements.txt` after you activate the env.

#### 1Â Â Install quantisation toolingÂ (TaskÂ 10)

```bash
pip install --upgrade "optimum[gptq]" gptqmodel
```

This pulls GPU wheels with bundled **ExLlamaâ€‘V2** & **TorchQuantLinear** kernelsâ€”no source build on Windows/Linux/macOS.

#### 2Â Â Download a readyâ€‘made 4â€‘bit checkpointÂ (TaskÂ 11)

```bash
huggingface-cli login                           # oneâ€‘time
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GPTQ \
  --local-dir models/mistral/gptq \
  --local-dir-use-symlinks False
```

* SizeÂ â‰ˆâ€¯4.2â€¯GB, groupâ€‘sizeÂ 128, Actâ€‘Order True.
* Folder **`models/` is gitâ€‘ignored**â€”weights stay out of the repo.

#### 3Â Â Smokeâ€‘test & latency benchmarkÂ (TaskÂ 12)

```bash
python smoke_mistral.py models/mistral/gptq -- --bench
```

* Cold load â‰ˆâ€¯3â€¯s on an RTXÂ 30â€‘series; hot reload â‰ˆâ€¯2â€¯s.
* ThroughputÂ â‰ˆâ€¯20â€‘30â€¯tokâ€¯/â€¯s with ExLlamaâ€‘V2 kernels.
* If you see only 3â€‘5â€¯tokâ€¯/â€¯s youâ€™re on CPUâ€”reâ€‘install the *cu121* wheels.

---

PhaseÂ 4 is now complete; proceed to **PhaseÂ 5Â â€“ RAG chain wiring**.
