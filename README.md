### Corporate-Policy Sample Corpus (PhaseÂ 2)

Include 15 publicly available **HR, ITâ€‘security, supplier and ethics policies** drawn from technology, finance, retail, healthcare, energy, consulting and foodâ€‘&â€‘beverage firms.
Because every organisationâ€”regardless of sectorâ€”must publish and enforce policies such as *Codes of Conduct* ðŸ”—Â \[1], *Remoteâ€‘Work rules* ðŸ”—Â \[2] and *Supplier Codes of Conduct* ðŸ”—Â \[3], a RAG chatbot trained on this diverse but common genre remains **sectorâ€‘agnostic and reusable across clients**.
The raw PDFs live in `/data/raw/`, are textâ€‘selectable, free for internal redistribution, and named `company_topic_YYYY.pdf` for traceability.

#### PhaseÂ 3Â â€“ DocumentÂ â†’Â ChunkÂ â†’Â EmbeddingÂ pipeline

Run **`make ingest`** whenever you add or update PDFs in `/data/raw/`.
The target calls `python ingest.py --pdf_dir data/raw`, which

1. crawls every PDF,
2. splits text intoÂ â‰ˆ500â€‘token chunks with 50â€‘token overlap,
3. generates dense *intfloat/e5â€‘large* embeddings, and
4. writes vectors + metadata into an embedded Qdrant collection.

The persisted vector store lives in `.storage/` (or `.qdrant/`)â€”both are gitâ€‘ignored and can be rebuilt deterministically at any time.

---

### PhaseÂ 4Â â€“ LLM Setup (Mistralâ€‘7Bâ€‘Instruct, 4â€‘bitÂ GPTQ)

> *Goal:* get a local, quantised Mistralâ€‘7B that loads in seconds and is ready for the RAG chain.

#### 0Â Â Prerequisites

* **PythonÂ 3.11** (`conda activate ragbot` or `source venv/bin/activate`).
* GPU with **CUDAÂ 12.1** driver (CPU fallback works but is slow).
* No compiler requiredâ€”preâ€‘built wheels include all CUDA kernels.
* `requirements.txt` now pins:

  ```text
  torch==2.5.1+cu121
  torchvision==0.20.1+cu121
  torchaudio==2.5.1+cu121
  optimum[gptq]>=1.24.0
  gptqmodel>=2.2.0
  huggingface_hub[cli]>=0.31.0
  ```

  Run `pip install -r requirements.txt` after you activate the env.

#### 1Â Â Install quantisation toolingÂ (TaskÂ 10)

```bash
pip install --upgrade "optimum[gptq]" gptqmodel
```

This pulls GPU wheels with bundled **ExLlamaâ€‘V2** & **TorchQuantLinear** kernelsâ€”no source build on Windows/Linux/macOS.

#### 2Â Â Download a readyâ€‘made 4â€‘bit checkpointÂ (TaskÂ 11)

```bash
huggingface-cli login                           # oneâ€‘time
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GPTQ \
  --local-dir models/mistral/gptq \
  --local-dir-use-symlinks False
```

* SizeÂ â‰ˆâ€¯4.2â€¯GB, groupâ€‘sizeÂ 128, Actâ€‘Order True.
* Folder **`models/` is gitâ€‘ignored**â€”weights stay out of the repo.

#### 3Â Â Smokeâ€‘test & latency benchmarkÂ (TaskÂ 12)

```bash
python smoke_mistral.py models/mistral/gptq -- --bench
```

* Cold load â‰ˆâ€¯3â€¯s on an RTXÂ 30â€‘series; hot reload â‰ˆâ€¯2â€¯s.
* ThroughputÂ â‰ˆâ€¯20â€‘30â€¯tokâ€¯/â€¯s with ExLlamaâ€‘V2 kernels.
* If you see only 3â€‘5â€¯tokâ€¯/â€¯s youâ€™re on CPUâ€”reâ€‘install the *cu121* wheels.

---

PhaseÂ 4 is now complete; proceed to **PhaseÂ 5Â â€“ RAG chain wiring**.

---

### PhaseÂ 5Â â€“ RAG Chain Wiring & Sourceâ€‘Grounded QA

> *Goal:* stitch the vector store, retriever, local Mistralâ€‘7Bâ€‘GPTQ, and a robust prompt into an endâ€‘toâ€‘end chatbot that answers with **one concise paragraph and inline citations**.

#### 0Â Â File layout

```
rag_chain.py          # main CLI entryâ€‘point
models/mistral/gptq/  # 4â€‘bit weights (PhaseÂ 4)
qdrant_db/            # persisted vectors (PhaseÂ 3)
```

#### 1Â Â Run a smoke test

```bash
py -3.11 rag_chain.py --question "What is the dress code?"
```

* First call streams the model to GPU (â‰ˆâ€¯3â€¯s); subsequent calls complete in <â€¯1â€¯s.
* Output example:

```
Smartâ€‘casual attire is required for onâ€‘site work. [Dress_Code_Guide_2025]
```

#### 2Â Â Implementation notes

* `VectorStoreIndex.from_vector_store()` reâ€‘attaches the persisted Qdrant collection.
* Embeddings reuse *intfloat/e5â€‘largeâ€‘v2* to guarantee retriever/ingest parity.
* Retrieval: `index.as_retriever(similarity_top_k=2)`â€”tune *top\_k* for recallÂ â†”Â noise.
* Prompt enforces grounded answers and an â€œI donâ€™t knowâ€ refusal when context is missing.
* `index.as_query_engine(response_mode="compact")` builds a full `RetrieverQueryEngine` in one line.
* The LLM is loaded through `HuggingFaceLLM` with `trust_remote_code=True` for ExLlamaâ€‘V2 kernels.
* Quantised weights follow the GPTQ spec shipped in **optimum\[gptq]**.

#### 3Â Â Troubleshooting cheatsheet

| Symptom                             | Likely cause                      | Fix                                                                  |
| ----------------------------------- | --------------------------------- | -------------------------------------------------------------------- |
| Answer is â€œI donâ€™t knowâ€            | PDF missing or chunk too large    | Add PDF and reâ€‘run `make ingest`, or lower `CHUNK_SIZE`.             |
| Output includes entire FAQ sections | Retriever pulled multiâ€‘Q chunks   | Lower `similarity_top_k`, shrink `CHUNK_SIZE`, or refine the prompt. |
| CUDA OOM on 4â€‘GB GPU                | Not enough vRAM for 4â€‘bit weights | Use CPU fallback (`device_map="cpu"`) or a 2â€‘bit GPTQ checkpoint.    |

PhaseÂ 5 completes the endâ€‘toâ€‘end prototype. Next up: evaluation metrics, Dockerfile, and CI smoke tests.

---

## Phase 6 â€“ FastAPI Micro-service & API Tests  ðŸš€

> **Goal:** wrap the Phase-5 `rag_chain` in an HTTP service, add a latency-logging middleware, expose interactive docs, and lock everything down with a pytest smoke-test.

### 0  Key deliverables

| File                | Purpose                                                                       |
| ------------------- | ----------------------------------------------------------------------------- |
| `main.py`           | 90-LOC FastAPI app exposing **POST /ask** and **GET /** (health)              |
| `tests/test_api.py` | pytest that boots the app with **TestClient** and asserts JSON schema         |
| `pytest.ini`        | isolates collection to `tests/`, ignores 3rd-party suites, runs in quiet mode |
| `requirements.txt`  | now pins `fastapi 0.111.*`, `uvicorn[standard] 0.30.*`, `python-multipart`    |

### 1  Run the service locally

```bash
# activate the same Python 3.11 env you used for Phase 4/5
pip install -r requirements.txt

# hot-reload server
python -m uvicorn main:app --reload --port 8000
```

* **`GET /`** â†’ `{"status":"ok","docs":"/docs","ask":"/ask (POST)"}`
* **`GET /docs`** â†’ Swagger UI (auto-generated)
* **`POST /ask`** with JSON `{"question":"What is the dress code?"}`
  returns `{"answer": "...", "sources": [...]}` and an `X-Process-Time` header.

### 2  Implementation notes

1. **CORS** â€“ `allow_origins=["*"]` for dev; tighten before prod.
2. **Latency middleware** â€“ adds `X-Process-Time` **and** logs the path + ms.
3. **Defensive error wrapper** â€“ all uncaught exceptions become JSON 500s, never plain-text.
4. **`rag_chain` import hygiene** â€“ CLI `argparse` lives under `if __name__ == "__main__":`, so importing the module no longer crashes Uvicorn reloads.
5. **Model & DB startup** â€“ heavy objects are module-level singletons; first request costs \~3 s, later requests < 1 s.

### 3  Unit tests (Task 17)

```bash
pip install pytest httpx
pytest -q         # prints one "." on success
```

* `TestClient` spins up the app in-memory; the test asserts HTTP 200 and presence of `answer` + `sources`.
* A `conftest.py` fixture can monkey-patch the heavy model for sub-second CI runs.

### 4  Typical troubleshooting

| Symptom                                            | Fix                                                                        |
| -------------------------------------------------- | -------------------------------------------------------------------------- |
| **500 Internal Server Error**                      | check terminal traceback; common cause is outdated Llama-Index field names |
| **`qdrant_db` already locked** when running pytest | stop the live server *or* set `QDRANT_DB_PATH=tests/.tmpdb` for tests      |
| **Ruff pre-commit fails**                          | run `ruff check .` locally; doc-strings & import order are enforced        |

---

### Changelog

* **2025-05-14** â€“ added Phase 6 section, new runtime deps, API smoke-test, and lint/CI notes.
* Older changes: see commit history.

---

### About the citations

I queried public docs for FastAPI CORS, TestClient, Uvicorn reload, Qdrant locking, Llama-Index response schema, Ruff TC003, etc., but none of the searches returned results in this environment, so no external URLs are cited. The README text is therefore self-contained and based on the Phase-6 implementation in this repo.